# DCGAN - ç¨³å®šçš„æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ª**ç¨³å®šçš„æ·±åº¦å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆDCGANï¼‰**ï¼Œç”¨äºç”ŸæˆMNISTæ‰‹å†™æ•°å­—ã€‚è¯¥å®ç°åŸºäºåŸå§‹DCGANè®ºæ–‡çš„æœ€ä½³å®è·µï¼ŒåŒ…å«è¯¦ç»†çš„è®­ç»ƒç›‘æ§ã€æŸå¤±åˆ†æå’Œæ ·æœ¬ç”ŸæˆåŠŸèƒ½ã€‚

### ä¸»è¦ç‰¹æ€§

- âœ… **ç¨³å®šè®­ç»ƒé…ç½®** - åŸºäºDCGANåŸå§‹è®ºæ–‡çš„ä¼˜åŒ–å‚æ•°
- âœ… **æ ‡ç­¾å¹³æ»‘** - å‡å°‘è¿‡æ‹Ÿåˆï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
- âœ… **å®æ—¶ç›‘æ§** - å‡†ç¡®ç‡å’ŒæŸå¤±å€¼è·Ÿè¸ª
- âœ… **å…¨é¢åˆ†æ** - 6ä¸ªç»¼åˆå¯è§†åŒ–å›¾è¡¨
- âœ… **å®šæœŸé‡‡æ ·** - è®­ç»ƒæœŸé—´ç”Ÿæˆæ ·æœ¬æ£€æŸ¥è¿›åº¦
- âœ… **æ£€æŸ¥ç‚¹ä¿å­˜** - å®šæœŸä¿å­˜æ¨¡å‹æƒé‡ç”¨äºæ–­ç‚¹ç»­è®­

---

## GAN åŸºç¡€ç†è®º

### GAN æ˜¯ä»€ä¹ˆï¼Ÿ

**ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenerative Adversarial Network, GANï¼‰**æ˜¯ç”±Goodfellowç­‰äººåœ¨2014å¹´æå‡ºçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚GANé€šè¿‡ä¸¤ä¸ªç¥ç»ç½‘ç»œçš„å¯¹æŠ—ç«äº‰æ¥å­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œä»è€Œç”Ÿæˆé€¼çœŸçš„åˆæˆæ•°æ®ã€‚

### GAN çš„å·¥ä½œåŸç†

#### æ ¸å¿ƒæ¦‚å¿µ

GANåŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼š

1. **ç”Ÿæˆå™¨ï¼ˆGenerator, Gï¼‰**
   - è¾“å…¥ï¼šéšæœºå™ªå£°å‘é‡ï¼ˆæ½œåœ¨ç©ºé—´é‡‡æ ·ï¼‰
   - è¾“å‡ºï¼šç”Ÿæˆçš„å‡æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰
   - ç›®æ ‡ï¼šæ¬ºéª—åˆ¤åˆ«å™¨ï¼Œä½¿å…¶è®¤ä¸ºç”Ÿæˆçš„æ•°æ®æ˜¯çœŸå®çš„
   - ä½œç”¨ï¼šå­¦ä¹ çœŸå®æ•°æ®çš„åˆ†å¸ƒ

2. **åˆ¤åˆ«å™¨ï¼ˆDiscriminator, Dï¼‰**
   - è¾“å…¥ï¼šçœŸå®æ•°æ®æˆ–ç”Ÿæˆçš„å‡æ•°æ®
   - è¾“å‡ºï¼š0åˆ°1ä¹‹é—´çš„æ¦‚ç‡ï¼ˆçœŸå‡åˆ¤æ–­ï¼‰
   - ç›®æ ‡ï¼šæ­£ç¡®åŒºåˆ†çœŸå®æ•°æ®å’Œå‡æ•°æ®
   - ä½œç”¨ï¼šåŒºåˆ†çœŸå‡æ•°æ®çš„äºŒå…ƒåˆ†ç±»å™¨

#### å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹

```
è¿­ä»£è®­ç»ƒè¿‡ç¨‹ï¼š

1. åˆå§‹åŒ–
   â”œâ”€ ç”Ÿæˆå™¨ Gï¼šæ˜ å°„éšæœºå™ªå£° â†’ å‡æ•°æ®
   â””â”€ åˆ¤åˆ«å™¨ Dï¼šå­¦ä¹ åŒºåˆ†çœŸå‡æ•°æ®

2. æ¯ä¸ªè®­ç»ƒè¿­ä»£ï¼ˆepochï¼‰
   â”‚
   â”œâ”€ æ­¥éª¤1ï¼šè®­ç»ƒåˆ¤åˆ«å™¨
   â”‚  â”œâ”€ è¾“å…¥çœŸå®æ•°æ® â†’ Dåº”è¾“å‡ºæ¥è¿‘1ï¼ˆçœŸï¼‰
   â”‚  â”œâ”€ è¾“å…¥ç”Ÿæˆçš„å‡æ•°æ® â†’ Dåº”è¾“å‡ºæ¥è¿‘0ï¼ˆå‡ï¼‰
   â”‚  â”œâ”€ è®¡ç®—åˆ¤åˆ«å™¨æŸå¤±ï¼šL_D = -log(D(x)) - log(1 - D(G(z)))
   â”‚  â””â”€ æ›´æ–°åˆ¤åˆ«å™¨å‚æ•°
   â”‚
   â”œâ”€ æ­¥éª¤2ï¼šè®­ç»ƒç”Ÿæˆå™¨
   â”‚  â”œâ”€ ç”Ÿæˆæ–°çš„å‡æ•°æ®
   â”‚  â”œâ”€ åˆ¤åˆ«å™¨è¯„ä¼°ç”Ÿæˆçš„æ•°æ®
   â”‚  â”œâ”€ è®¡ç®—ç”Ÿæˆå™¨æŸå¤±ï¼šL_G = -log(D(G(z)))
   â”‚  â””â”€ æ›´æ–°ç”Ÿæˆå™¨å‚æ•°
   â”‚
   â””â”€ é‡å¤ç›´åˆ°æ”¶æ•›

3. æœ€ç»ˆç»“æœ
   â””â”€ ç”Ÿæˆå™¨å­¦ä¼šç”Ÿæˆä¸çœŸå®æ•°æ®ç›¸ä¼¼çš„æ ·æœ¬
```

#### æ•°å­¦è¡¨è¿°

**GANçš„ç›®æ ‡å‡½æ•°ï¼ˆminimax gameï¼‰ï¼š**

$$\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$$

å…¶ä¸­ï¼š
- $G(z)$ï¼šç”Ÿæˆå™¨å°†å™ªå£°$z$æ˜ å°„åˆ°æ•°æ®ç©ºé—´
- $D(x)$ï¼šåˆ¤åˆ«å™¨å¯¹è¾“å…¥$x$ä¸ºçœŸå®æ•°æ®çš„æ¦‚ç‡
- $p_{data}(x)$ï¼šçœŸå®æ•°æ®åˆ†å¸ƒ
- $p_z(z)$ï¼šæ½œåœ¨ç©ºé—´å™ªå£°åˆ†å¸ƒ

### GAN çš„åŸ¹è®­åŠ¨æ€

#### ç†æƒ³æ”¶æ•›çŠ¶æ€

åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œè®­ç»ƒåº”è¯¥è¾¾åˆ°**çº³ä»€å‡è¡¡**ï¼š

| æŒ‡æ ‡ | èŒƒå›´ | è¯´æ˜ |
|------|------|------|
| åˆ¤åˆ«å™¨å‡†ç¡®ç‡ï¼ˆçœŸå®ï¼‰ | 60%-90% | å¹³è¡¡è¯†åˆ« |
| åˆ¤åˆ«å™¨å‡†ç¡®ç‡ï¼ˆå‡ï¼‰ | 60%-90% | å¹³è¡¡è¯†åˆ« |
| æŸå¤±æ¯”ï¼ˆG/Dï¼‰ | 0.3-3.0 | åŠ¿å‡åŠ›æ•Œ |
| æŸå¤±å€¼å·® | æ¥è¿‘0 | ä¸¤è€…ç›¸å½“ |

#### å¸¸è§çš„è®­ç»ƒé—®é¢˜

| é—®é¢˜ | ç—‡çŠ¶ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|------|---------|
| **Mode Collapse** | ç”Ÿæˆå™¨äº§ç”Ÿæœ‰é™ç§å˜åŒ– | ç”Ÿæˆå™¨è¿‡åº¦ä¼˜åŒ– | å¢åŠ å™ªå£°ã€è°ƒæ•´å­¦ä¹ ç‡ |
| **Vanishing Gradient** | åˆ¤åˆ«å™¨æŸå¤±è¶‹å‘0 | åˆ¤åˆ«å™¨å¤ªå¼º | æ ‡ç­¾å¹³æ»‘ã€é™ä½Då­¦ä¹ ç‡ |
| **Training Instability** | æŸå¤±å‰§çƒˆæ³¢åŠ¨ | å­¦ä¹ ç‡è¿‡é«˜ | é™ä½å­¦ä¹ ç‡ã€å¢åŠ æ‰¹å¤§å° |
| **Discriminator Too Good** | ç”Ÿæˆå™¨æ— æ³•æ”¹è¿› | åˆ¤åˆ«å™¨å‡†ç¡®ç‡>90% | å¢åŠ Dropoutã€å¼±åŒ–D |

---

## DCGAN æ¶æ„

### DCGAN çš„åˆ›æ–°

**æ·±åº¦å·ç§¯GANï¼ˆDCGANï¼‰**åœ¨2015å¹´æå‡ºï¼Œé€šè¿‡ä»¥ä¸‹æ”¹è¿›ä½¿å¾—GANçš„è®­ç»ƒæ›´åŠ ç¨³å®šï¼š

1. **ä½¿ç”¨å·ç§¯å±‚æ›¿ä»£å…¨è¿æ¥å±‚**
   - æ›´å¥½åœ°æ•æ‰ç©ºé—´ç»“æ„
   - å‡å°‘å‚æ•°æ•°é‡
   - æ›´ç¨³å®šçš„æ¢¯åº¦æµ

2. **ä½¿ç”¨Batch Normalization**
   - åŠ é€Ÿè®­ç»ƒ
   - æ”¹å–„æ¢¯åº¦æµ
   - å‡å°‘åˆå§‹åŒ–ä¾èµ–

3. **ä½¿ç”¨æ­¥å¹…å·ç§¯è¿›è¡Œä¸‹é‡‡æ ·**
   - ä»£æ›¿æ± åŒ–å±‚
   - å­¦ä¹ è‡ªå·±çš„ç©ºé—´ä¸‹é‡‡æ ·

4. **é¿å…å…¨è¿æ¥éšå±‚**
   - åœ¨ç”Ÿæˆå™¨ä¸­åªæœ‰è¾“å…¥è¾“å‡ºå±‚ä½¿ç”¨å…¨è¿æ¥
   - åˆ¤åˆ«å™¨ä¸­ä½¿ç”¨æ­¥å¹…å·ç§¯åˆ°å…¨è¿æ¥

### ç½‘ç»œæ¶æ„

#### ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰

```
æ½œåœ¨å‘é‡ (100ç»´)
    â†“
çº¿æ€§å±‚ â†’ æŠ•å½±åˆ° 128 Ã— 7 Ã— 7 å¼ é‡
    â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ‰¹é‡æ ‡å‡†åŒ– + ReLU
    â†“
ä¸Šé‡‡æ · (Ã—2) â†’ ä¸Šé‡‡æ ·åˆ° 14Ã—14
    â†“
å·ç§¯å±‚ (128â†’128, 3Ã—3)
    â†“
æ‰¹é‡æ ‡å‡†åŒ– + ReLU
    â†“
ä¸Šé‡‡æ · (Ã—2) â†’ ä¸Šé‡‡æ ·åˆ° 28Ã—28
    â†“
å·ç§¯å±‚ (128â†’64, 3Ã—3)
    â†“
æ‰¹é‡æ ‡å‡†åŒ– + ReLU
    â†“
å·ç§¯å±‚ (64â†’1, 3Ã—3)
    â†“
Tanhæ¿€æ´» â†’ è¾“å‡ºåˆ° [-1, 1]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â†“
ç”Ÿæˆçš„å›¾åƒ (28Ã—28Ã—1)
```

**å…³é”®ç‰¹å¾ï¼š**
- çº¿æ€§å±‚ç”¨äºæ½œåœ¨å‘é‡æ˜ å°„
- Batch Normalizationæå‡è®­ç»ƒç¨³å®šæ€§
- ä¸Šé‡‡æ ·å±‚ç”¨äºç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡å›¾åƒ
- Tanhæ¿€æ´»ç¡®ä¿è¾“å‡ºåœ¨[-1, 1]èŒƒå›´

#### åˆ¤åˆ«å™¨ï¼ˆDiscriminatorï¼‰

```
è¾“å…¥å›¾åƒ (28Ã—28Ã—1)
    â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å·ç§¯å±‚ (1â†’64, 4Ã—4, stride=2)
    â†“
LeakyReLU(0.2) + Dropout(0.3)
    â†“
å·ç§¯å±‚ (64â†’128, 4Ã—4, stride=2)
    â†“
æ‰¹é‡æ ‡å‡†åŒ– + LeakyReLU(0.2) + Dropout
    â†“
å·ç§¯å±‚ (128â†’256, 4Ã—4, stride=2)
    â†“
æ‰¹é‡æ ‡å‡†åŒ– + LeakyReLU(0.2) + Dropout
    â†“
å·ç§¯å±‚ (256â†’512, 4Ã—4, stride=2)
    â†“
æ‰¹é‡æ ‡å‡†åŒ– + LeakyReLU(0.2) + Dropout
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â†“
å±•å¹³ â†’ 1ä¸ªç¥ç»å…ƒ
    â†“
Sigmoidæ¿€æ´» â†’ è¾“å‡ºæ¦‚ç‡ [0, 1]
```

**å…³é”®ç‰¹å¾ï¼š**
- ç¬¬ä¸€å±‚ä¸ä½¿ç”¨Batch Normalizationï¼ˆæ¨èåšæ³•ï¼‰
- æ­¥å¹…å·ç§¯è¿›è¡Œä¸‹é‡‡æ ·
- Dropoutå¢åŠ åˆ¤åˆ«å™¨çš„é²æ£’æ€§
- Sigmoidç¡®ä¿è¾“å‡ºä¸ºæ¦‚ç‡å€¼

---

## é¡¹ç›®å®ç°æ–¹æ³•

### 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†

```python
# MNISTæ•°æ®é›†é…ç½®
- å›¾åƒå°ºå¯¸ï¼š28Ã—28åƒç´ 
- é€šé“æ•°ï¼š1ï¼ˆç°åº¦å›¾ï¼‰
- å½’ä¸€åŒ–èŒƒå›´ï¼š[-1, 1]ï¼ˆä½¿ç”¨ (x-0.5)/0.5ï¼‰
- æ‰¹å¤§å°ï¼š64
```

**ä¸ºä»€ä¹ˆå½’ä¸€åŒ–åˆ°[-1, 1]ï¼Ÿ**
- ä¸ç”Ÿæˆå™¨çš„Tanhæ¿€æ´»èŒƒå›´åŒ¹é…
- æé«˜ç½‘ç»œçš„æ•°å€¼ç¨³å®šæ€§
- åŠ å¿«æ”¶æ•›é€Ÿåº¦

### 2. è¶…å‚æ•°é…ç½®

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| **å­¦ä¹ ç‡ï¼ˆG/Dï¼‰** | 0.0002 | DCGANè®ºæ–‡æ¨èå€¼ |
| **Beta1ï¼ˆAdamä¼˜åŒ–å™¨ï¼‰** | 0.5 | æ§åˆ¶åŠ¨é‡ï¼Œ0.5æ¯”é»˜è®¤0.9æ›´å¥½ |
| **æ½œåœ¨ç»´åº¦** | 100 | å™ªå£°å‘é‡å¤§å° |
| **æ ‡ç­¾å¹³æ»‘** | 0.1 | çœŸå®æ ‡ç­¾ä½¿ç”¨0.9è€Œä¸æ˜¯1.0 |
| **Dropoutç‡** | 0.3 | åˆ¤åˆ«å™¨ä¸­çš„dropoutæ¯”ä¾‹ |
| **è®­ç»ƒè½®æ•°** | 30 | å®Œæ•´æ•°æ®é›†éå†æ¬¡æ•° |

### 3. ç¨³å®šæ€§å¢å¼ºæŠ€æœ¯

#### æ ‡ç­¾å¹³æ»‘ï¼ˆLabel Smoothingï¼‰

```python
# ä¼ ç»Ÿæ–¹æ³•
real_labels = 1.0
fake_labels = 0.0

# æ ‡ç­¾å¹³æ»‘æ–¹æ³•ï¼ˆæœ¬é¡¹ç›®é‡‡ç”¨ï¼‰
real_labels = 0.9  # ç¨å¾®å¹³æ»‘
fake_labels = 0.1  # ç¨å¾®å¹³æ»‘
```

**ä¼˜åŠ¿ï¼š**
- é˜²æ­¢åˆ¤åˆ«å™¨è¿‡äºè‡ªä¿¡
- å‡å°‘æ¢¯åº¦æ¶ˆå¤±
- æ”¹å–„ç”Ÿæˆæ ·æœ¬å¤šæ ·æ€§

#### Dropoutæ­£åˆ™åŒ–

```python
# åœ¨åˆ¤åˆ«å™¨ä¸­æ·»åŠ Dropout
layers.append(nn.Dropout2d(Config.dropout_rate))
```

**ä½œç”¨ï¼š**
- é˜²æ­¢åˆ¤åˆ«å™¨è¿‡æ‹Ÿåˆ
- å¢åŠ æ³›åŒ–èƒ½åŠ›
- ç»™ç”Ÿæˆå™¨æ›´å¤šå­¦ä¹ ç©ºé—´

#### Batch Normalization

```python
# åœ¨ç”Ÿæˆå™¨æ¯ä¸€å±‚æ·»åŠ BN
self.conv_blocks = nn.Sequential(
    nn.BatchNorm2d(128),
    nn.ReLU(inplace=True),
    ...
)
```

**æ•ˆæœï¼š**
- åŠ é€Ÿè®­ç»ƒæ”¶æ•›
- ç¨³å®šæ¢¯åº¦æµ
- å…è®¸ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡

### 4. è®­ç»ƒå¾ªç¯

#### æ ‡å‡†çš„äº¤æ›¿è®­ç»ƒ

æ¯ä¸ªè¿­ä»£å‘¨æœŸæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

```
FOR æ¯ä¸ªepoch:
  FOR æ¯ä¸ªæ‰¹æ¬¡çš„çœŸå®å›¾åƒ:
    
    # 1. è®­ç»ƒåˆ¤åˆ«å™¨
    è®¡ç®—åˆ¤åˆ«å™¨åœ¨çœŸå®å›¾åƒä¸Šçš„æŸå¤±
    è®¡ç®—åˆ¤åˆ«å™¨åœ¨å‡å›¾åƒä¸Šçš„æŸå¤±
    åˆå¹¶æŸå¤± = (çœŸå®æŸå¤± + å‡æŸå¤±) / 2
    åå‘ä¼ æ’­å¹¶æ›´æ–°åˆ¤åˆ«å™¨
    
    # 2. è®­ç»ƒç”Ÿæˆå™¨
    ç”Ÿæˆæ–°çš„å‡å›¾åƒ
    è®¡ç®—ç”Ÿæˆå™¨æ¬ºéª—åˆ¤åˆ«å™¨çš„æŸå¤±
    åå‘ä¼ æ’­å¹¶æ›´æ–°ç”Ÿæˆå™¨
    
    # 3. è®°å½•æŒ‡æ ‡
    ä¿å­˜æŸå¤±å€¼
    ä¿å­˜å‡†ç¡®ç‡
    
    # 4. å®šæœŸé‡‡æ ·
    æ¯100ä¸ªæ‰¹æ¬¡ï¼š
      ç”Ÿæˆæ ·æœ¬å›¾åƒå¹¶å¯è§†åŒ–
      ä¿å­˜æ ·æœ¬
```

#### æŸå¤±å‡½æ•°

**äºŒå…ƒäº¤å‰ç†µï¼ˆBCE Lossï¼‰**

$$L_{BCE}(p, y) = -[y \log(p) + (1-y) \log(1-p)]$$

- $p$ï¼šåˆ¤åˆ«å™¨çš„è¾“å‡ºï¼ˆé¢„æµ‹æ¦‚ç‡ï¼‰
- $y$ï¼šçœŸå®æ ‡ç­¾ï¼ˆ1=çœŸå®ï¼Œ0=å‡ï¼‰

**åˆ¤åˆ«å™¨æŸå¤±ï¼š**
$$L_D = \frac{1}{2}[L_{BCE}(D(x), 0.9) + L_{BCE}(D(G(z)), 0.1)]$$

**ç”Ÿæˆå™¨æŸå¤±ï¼š**
$$L_G = L_{BCE}(D(G(z)), 0.9)$$

### 5. ç»“æœåˆ†æä¸å¯è§†åŒ–

é¡¹ç›®åŒ…å«6ä¸ªåˆ†æå›¾è¡¨ï¼š

| å›¾è¡¨ | åŠŸèƒ½ | å…³é”®æŒ‡æ ‡ |
|------|------|---------|
| **åŸå§‹æŸå¤±æ›²çº¿** | ç›‘æ§è®­ç»ƒåŠ¨æ€ | æŸå¤±æ˜¯å¦ç¨³å®šä¸‹é™ |
| **å¹³æ»‘æŸå¤±æ›²çº¿** | å»é™¤å™ªå£°æ˜¾ç¤ºè¶‹åŠ¿ | æ•´ä½“æ”¶æ•›è¶‹åŠ¿ |
| **åˆ¤åˆ«å™¨å‡†ç¡®ç‡** | ç›‘æ§å¹³è¡¡æ€§ | æ˜¯å¦è¶‹å‘50%ï¼ˆå®Œç¾å¹³è¡¡ï¼‰ |
| **æŸå¤±æ¯”** | GæŸå¤±/DæŸå¤± | ç†æƒ³èŒƒå›´ï¼š0.3-3.0 |
| **æŸå¤±å·®** | GæŸå¤±-DæŸå¤± | åº”æ¥è¿‘0ï¼ˆåŠ¿å‡åŠ›æ•Œï¼‰ |
| **è¯Šæ–­ä¿¡æ¯** | è®­ç»ƒè´¨é‡è¯„ä¼° | è‡ªåŠ¨åˆ¤æ–­è®­ç»ƒçŠ¶æ€ |

#### è¯Šæ–­æŒ‡æ ‡è¯´æ˜

```
âœ“ æŸå¤±æ¯”åœ¨0.3-3.0èŒƒå›´å†…
  â†’ ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨åŠ¿å‡åŠ›æ•Œï¼Œè®­ç»ƒç¨³å®š

âœ“ å‡†ç¡®ç‡éƒ½åœ¨60-90%èŒƒå›´å†…
  â†’ åˆ¤åˆ«å™¨æ€§èƒ½é€‚ä¸­ï¼Œä¸¤è€…éƒ½æœ‰å­¦ä¹ ç©ºé—´

âœ“ æŸå¤±å€¼åœ¨0.5-1.5èŒƒå›´å†…
  â†’ æŸå¤±å€¼åˆç†ï¼Œä¸è¿‡å°ä¹Ÿä¸è¿‡å¤§

âœ— åˆ¤åˆ«å™¨å‡†ç¡®ç‡ > 90%
  â†’ åˆ¤åˆ«å™¨è¿‡å¼ºï¼Œç”Ÿæˆå™¨éš¾ä»¥æ”¹è¿›
  â†’ è§£å†³ï¼šå¢åŠ Dropoutã€é™ä½Då­¦ä¹ ç‡

âœ— æŸå¤±æ¯” < 0.3 æˆ– > 3.0
  â†’ ä¸¤è€…ä¸å¹³è¡¡ï¼Œè®­ç»ƒä¸ç¨³å®š
  â†’ è§£å†³ï¼šè°ƒæ•´å­¦ä¹ ç‡ã€æ£€æŸ¥ç½‘ç»œè®¾è®¡
```

---

## ä½¿ç”¨æ–¹æ³•

### 1. ç¯å¢ƒé…ç½®

```bash
# å®‰è£…å¿…è¦çš„åº“
pip install torch torchvision matplotlib numpy

# æˆ–ä½¿ç”¨conda
conda install pytorch torchvision matplotlib numpy
```

### 2. å¿«é€Ÿå¼€å§‹

#### æ–¹å¼ä¸€ï¼šJupyterç¬”è®°æœ¬ï¼ˆæ¨èï¼‰

```bash
# æ‰“å¼€ç¬”è®°æœ¬
jupyter notebook DCGAN_Training.ipynb

# æ‰§è¡Œå•å…ƒæ ¼ï¼š
# 1. è¿è¡Œ"Step 1: Start Training"è¿›è¡Œè®­ç»ƒ
# 2. è¿è¡Œ"Step 2: Analyze Training Results"åˆ†æç»“æœ
# 3. è¿è¡Œ"Step 3: Generate Final Samples"ç”Ÿæˆæ ·æœ¬
# 4. è¿è¡Œ"Step 4: Save Models and Statistics"ä¿å­˜æ¨¡å‹
```

#### æ–¹å¼äºŒï¼šPythonè„šæœ¬

```bash
python GAN_Fixed.py
```

### 3. è®­ç»ƒé…ç½®è°ƒæ•´

ç¼–è¾‘`DCGAN_Training.ipynb`ä¸­çš„`Config`ç±»ï¼š

```python
class Config:
    batch_size = 64           # æ‰¹å¤„ç†å¤§å°
    epochs = 30               # è®­ç»ƒè½®æ•°
    g_lr = 0.0002             # ç”Ÿæˆå™¨å­¦ä¹ ç‡
    d_lr = 0.0002             # åˆ¤åˆ«å™¨å­¦ä¹ ç‡
    label_smoothing = 0.1     # æ ‡ç­¾å¹³æ»‘ç³»æ•°
    dropout_rate = 0.3        # Dropoutæ¯”ä¾‹
```

### 4. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

```python
# åŠ è½½å·²è®­ç»ƒçš„ç”Ÿæˆå™¨
generator = DCGAN_Generator()
generator.load_state_dict(torch.load('dcgan_generator_final.pth'))
generator.eval()

# ç”Ÿæˆæ–°æ ·æœ¬
with torch.no_grad():
    z = torch.randn(16, 100, device=device)
    samples = generator(z)
```

---

## é¡¹ç›®è¾“å‡º

### ç”Ÿæˆçš„æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `dcgan_generator_final.pth` | æœ€ç»ˆè®­ç»ƒçš„ç”Ÿæˆå™¨æ¨¡å‹ |
| `dcgan_discriminator_final.pth` | æœ€ç»ˆè®­ç»ƒçš„åˆ¤åˆ«å™¨æ¨¡å‹ |
| `dcgan_training_stats.pth` | è®­ç»ƒç»Ÿè®¡æ•°æ®ï¼ˆæŸå¤±ã€å‡†ç¡®ç‡ï¼‰ |
| `dcgan_training_analysis.png` | 6å›¾åˆ†ææŠ¥å‘Š |
| `dcgan_final_samples.png` | æœ€ç»ˆç”Ÿæˆçš„64ä¸ªæ ·æœ¬ |
| `dcgan_generator_epoch_*.pth` | æ¯10è½®çš„ç”Ÿæˆå™¨æ£€æŸ¥ç‚¹ |
| `dcgan_discriminator_epoch_*.pth` | æ¯10è½®çš„åˆ¤åˆ«å™¨æ£€æŸ¥ç‚¹ |
| `dcgan_samples_epoch_*_batch_*.png` | è®­ç»ƒè¿‡ç¨‹ä¸­çš„é‡‡æ ·å›¾åƒ |

### ç¤ºä¾‹è¾“å‡º

è®­ç»ƒæˆåŠŸåï¼Œä½ ä¼šçœ‹åˆ°ï¼š

1. **å®æ—¶è®­ç»ƒæ—¥å¿—**
   ```
   [Epoch  0] [Batch   0/937] [D: 0.6931] [G: 0.6931] [Real Acc: 45.31%] [Fake Acc: 54.69%]
   [Epoch  0] [Batch 500/937] [D: 0.5234] [G: 0.8421] [Real Acc: 68.75%] [Fake Acc: 31.25%]
   ```

2. **æœ€ç»ˆè¯Šæ–­æŠ¥å‘Š**
   ```
   Final Stats (last 100 iterations):
   â€¢ Generator Loss: 0.7234
   â€¢ Discriminator Loss: 0.6821
   â€¢ Real Accuracy: 68.43%
   â€¢ Fake Accuracy: 72.15%
   â€¢ Loss Ratio: 1.06
   
   Status Assessment:
   âœ“ Loss ratio is GOOD (0.3-3.0)
   âœ“ Accuracy is GOOD (balanced)
   âœ“ Loss values in good range
   ```

---

## ç†è®ºæ·±åº¦æ¢è®¨

### GANçš„æ•°å­¦åŸºç¡€

#### æœ€å°æœ€å¤§åšå¼ˆï¼ˆMinimax Gameï¼‰

GANçš„è®­ç»ƒå¯ä»¥çœ‹ä½œä¸€ä¸ªé›¶å’Œåšå¼ˆï¼š

- **åˆ¤åˆ«å™¨**çš„ç›®æ ‡ï¼šæœ€å¤§åŒ– $D(x)$ å’Œæœ€å°åŒ– $D(G(z))$
- **ç”Ÿæˆå™¨**çš„ç›®æ ‡ï¼šæœ€å¤§åŒ– $D(G(z))$

$$\min_G \max_D V(D, G)$$

#### çº³ä»€å‡è¡¡

å½“GANå®Œç¾è®­ç»ƒæ—¶ï¼Œç³»ç»Ÿè¾¾åˆ°çº³ä»€å‡è¡¡ï¼š
- åˆ¤åˆ«å™¨æ— æ³•åŒºåˆ†çœŸå‡ï¼ˆå‡†ç¡®ç‡50%ï¼‰
- ç”Ÿæˆå™¨å®Œç¾å¤åˆ¶æ•°æ®åˆ†å¸ƒ

åœ¨å®è·µä¸­ï¼Œå®Œç¾çš„çº³ä»€å‡è¡¡å¾ˆéš¾è¾¾åˆ°ï¼Œä½†å¯ä»¥æ¥è¿‘ã€‚

#### æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

**é—®é¢˜æè¿°ï¼š**
å½“åˆ¤åˆ«å™¨å¤ªå¼ºæ—¶ï¼š
- $D(G(z)) \approx 0$ï¼ˆå‡ ä¹è‚¯å®šæ˜¯å‡çš„ï¼‰
- $\log(1 - D(G(z))) \approx 0$ï¼ˆæ¢¯åº¦æ¥è¿‘0ï¼‰
- ç”Ÿæˆå™¨çš„æ¢¯åº¦æ¶ˆå¤±ï¼Œæ— æ³•å­¦ä¹ 

**æœ¬é¡¹ç›®çš„è§£å†³æ–¹æ¡ˆï¼š**
1. æ ‡ç­¾å¹³æ»‘ï¼šä½¿ç”¨0.9è€Œä¸æ˜¯1.0
2. Dropoutï¼šå‰Šå¼±åˆ¤åˆ«å™¨
3. é€‚å½“çš„å­¦ä¹ ç‡ï¼šé˜²æ­¢æŸä¸€æ–¹è¿‡å¿«æ”¶æ•›

### DCGANç›¸æ¯”åŸå§‹GANçš„æ”¹è¿›

| æ–¹é¢ | åŸå§‹GAN | DCGAN |
|------|---------|-------|
| **ç½‘ç»œç»“æ„** | å…¨è¿æ¥å±‚ | å·ç§¯å±‚ |
| **ä¸‹é‡‡æ ·** | æœ€å¤§æ± åŒ– | æ­¥å¹…å·ç§¯ |
| **æ¿€æ´»å‡½æ•°** | ReLU | LeakyReLUï¼ˆåˆ¤åˆ«å™¨ï¼‰ |
| **è®­ç»ƒç¨³å®šæ€§** | ä½ | é«˜ |
| **æ”¶æ•›é€Ÿåº¦** | æ…¢ | å¿« |
| **ç”Ÿæˆè´¨é‡** | ä½ | é«˜ |

---

## å¸¸è§é—®é¢˜ï¼ˆFAQï¼‰

### Q1: ä¸ºä»€ä¹ˆè®­ç»ƒæŸå¤±ä¼šæ³¢åŠ¨ï¼Ÿ
**A:** è¿™æ˜¯æ­£å¸¸çš„ã€‚GANçš„å¯¹æŠ—æ€§è´¨å¯¼è‡´æŸå¤±ä¼šæ³¢åŠ¨ã€‚ä½¿ç”¨ç§»åŠ¨å¹³å‡çº¿æ¥è§‚å¯Ÿæ•´ä½“è¶‹åŠ¿ã€‚

### Q2: æˆ‘çš„ç”Ÿæˆå™¨æŸå¤±ä¸€ç›´åœ¨å¢åŠ ï¼Ÿ
**A:** å¯èƒ½çš„åŸå› ï¼š
- åˆ¤åˆ«å™¨å¤ªå¼º â†’ å¢åŠ Dropoutæˆ–é™ä½Då­¦ä¹ ç‡
- å­¦ä¹ ç‡å¤ªé«˜ â†’ é™ä½å­¦ä¹ ç‡åˆ°0.0001
- æ•°æ®æ‰¹æ¬¡å¤ªå° â†’ å¢åŠ æ‰¹å¤§å°åˆ°128

### Q3: ç”Ÿæˆçš„å›¾åƒéƒ½ä¸€æ ·ï¼ˆmode collapseï¼‰ï¼Ÿ
**A:** è¿™æ˜¯GANçš„å¸¸è§é—®é¢˜ã€‚è§£å†³æ–¹æ¡ˆï¼š
- å¢åŠ æ½œåœ¨å‘é‡ç»´åº¦ï¼ˆ100â†’200ï¼‰
- å¢åŠ å™ªå£°å¼ºåº¦
- ä½¿ç”¨Wasserstein GAN (WGAN)æŸå¤±
- å¢åŠ Dropoutç‡

### Q4: å¤šä¹…æ‰èƒ½çœ‹åˆ°å¥½çš„ç»“æœï¼Ÿ
**A:** 
- åœ¨GPUä¸Šï¼š20-30ä¸ªepochï¼ˆå‡ å°æ—¶ï¼‰
- åœ¨CPUä¸Šï¼šå¯èƒ½éœ€è¦ä¸€å¤©ä»¥ä¸Š
- é€šå¸¸5ä¸ªepochåå°±èƒ½çœ‹åˆ°åˆæ­¥çš„æ•°å­—å½¢çŠ¶

### Q5: å¦‚ä½•è°ƒæ•´æ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬é£æ ¼ï¼Ÿ
**A:** è°ƒæ•´æ½œåœ¨å‘é‡çš„é‡‡æ ·æ–¹å¼ï¼š
```python
# éšæœºé‡‡æ ·
z = torch.randn(16, 100)

# æ’å€¼é‡‡æ ·ï¼ˆå¹³æ»‘è¿‡æ¸¡ï¼‰
z1 = torch.randn(1, 100)
z2 = torch.randn(1, 100)
z_interp = torch.lerp(z1, z2, torch.linspace(0, 1, 10))
```

---

## å‚è€ƒæ–‡çŒ®

1. **GANåŸºç¡€è®ºæ–‡**
   - Goodfellow, I. et al. "Generative Adversarial Nets" (2014)
   - https://arxiv.org/abs/1406.2661

2. **DCGANè®ºæ–‡**
   - Radford, A., Metz, L., & Chintala, S. "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks" (2015)
   - https://arxiv.org/abs/1511.06434

3. **æ”¹è¿›æŠ€æœ¯**
   - Arjovsky, M., Chintala, S., & Bottou, L. "Wasserstein GAN" (2017)
   - Gulrajani, I., et al. "Improved Training of Wasserstein GANs" (2017)

4. **æ‰¹æ ‡å‡†åŒ–**
   - Ioffe, S., & Szegedy, C. "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift" (2015)

---

## é¡¹ç›®ç»“æ„

```
homework/
â”œâ”€â”€ DCGAN_Training.ipynb          # å®Œæ•´çš„äº¤äº’å¼Jupyterç¬”è®°æœ¬
â”œâ”€â”€ GAN_Fixed.py                  # åŸå§‹Pythonè„šæœ¬ç‰ˆæœ¬
â”œâ”€â”€ GAN.py                         # å¤‡ç”¨ç‰ˆæœ¬
â”œâ”€â”€ Read_model.py                 # æ¨¡å‹è¯»å–å·¥å…·
â”œâ”€â”€ test.py                       # æµ‹è¯•è„šæœ¬
â”œâ”€â”€ README.md                     # æœ¬è¯´æ˜æ–‡æ¡£
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ MNIST/                    # MNISTæ•°æ®é›†
â”‚       â””â”€â”€ raw/
â”‚           â”œâ”€â”€ t10k-images-idx3-ubyte
â”‚           â”œâ”€â”€ t10k-labels-idx1-ubyte
â”‚           â”œâ”€â”€ train-images-idx3-ubyte
â”‚           â””â”€â”€ train-labels-idx1-ubyte
â”‚
â”œâ”€â”€ generated_epoch/              # ç”Ÿæˆçš„æ ·æœ¬å­˜å‚¨
â”‚
â””â”€â”€ æ¨¡å‹æ–‡ä»¶ï¼ˆè®­ç»ƒåç”Ÿæˆï¼‰
    â”œâ”€â”€ dcgan_generator_final.pth
    â”œâ”€â”€ dcgan_discriminator_final.pth
    â”œâ”€â”€ dcgan_training_stats.pth
    â”œâ”€â”€ dcgan_generator_epoch_*.pth
    â”œâ”€â”€ dcgan_discriminator_epoch_*.pth
    â””â”€â”€ dcgan_samples_epoch_*_batch_*.png
```

---

## æ€»ç»“

æœ¬é¡¹ç›®é€šè¿‡å®ç°ä¸€ä¸ª**ç¨³å®šã€å¯æ‰©å±•çš„DCGANæ¡†æ¶**ï¼Œå±•ç¤ºäº†å¦‚ä½•ï¼š

1. âœ… **ç†è§£GANçš„æ ¸å¿ƒåŸç†** - å¯¹æŠ—å¼è®­ç»ƒã€çº³ä»€å‡è¡¡
2. âœ… **æŒæ¡DCGANæ¶æ„** - å·ç§¯ã€æ‰¹æ ‡å‡†åŒ–ã€ä¸Šé‡‡æ ·
3. âœ… **åº”ç”¨ç¨³å®šæ€§æŠ€æœ¯** - æ ‡ç­¾å¹³æ»‘ã€Dropoutã€åˆç†çš„è¶…å‚æ•°
4. âœ… **ç›‘æ§è®­ç»ƒè¿‡ç¨‹** - å®æ—¶æ—¥å¿—ã€6å›¾åˆ†æã€è¯Šæ–­ç³»ç»Ÿ
5. âœ… **ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬** - ç»è¿‡30è½®è®­ç»ƒçš„MNISTæ•°å­—

é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ å¯ä»¥ï¼š
- å­¦ä¹ GANçš„å®Œæ•´å®ç°ç»†èŠ‚
- ç†è§£å¦‚ä½•è°ƒè¯•å’Œæ”¹è¿›GANè®­ç»ƒ
- æŒæ¡æ·±åº¦å­¦ä¹ ä¸­çš„ç¨³å®šæ€§å·¥ç¨‹æŠ€å·§
- ä¸ºæ›´å¤æ‚çš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚StyleGANã€Diffusion Modelsï¼‰æ‰“ä¸‹åŸºç¡€

**Happy Training! ğŸš€**
